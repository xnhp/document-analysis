\documentclass[10pt,twocolumn]{article}


\usepackage[utf8]{inputenc}

\usepackage{lecture-notes-template/zsfgv}
\usepackage{lipsum}
\usepackage{multicol}

\graphicspath{{img/}}

\raggedbottom

\begin{document}


{\Huge \vspace{1em}~\\ \textbf{Document Analysis}}

\tableofcontents

\part{Natural Language Processing}

\section{Introduction}

\paragraph{\textit{Challenges}}
\begin{itemize}
\item Complicated structure in sentence
\item Syntactic ambuigities (``\textit{time flies like an arrow}'',
  ``\textit{get the cat with the gloves}'').
\item Metaphores, humor, irony, ...
\item Semantics can be very rich and dependent on context, not easy to
  distuingish
\item Language requries knowledge about the world
\item Hard to really formalise the notion of ``meaning''
\end{itemize}

\paragraph{\textit{Basic approach}} Gather information on word based on its
context. Given a large text corpus, we can assume this to be meaningful
statistics.

\paragraph{\ild{Zipf's Law}} The number of elements with a given frequency
follows a power law distribution. That is, there is a small number of elements
which appear very often and the majority of elements appears rarely. In its most
simple form, the probability of the $n$-th most common word $p(n)$ is
\begin{align*}
  p(n) = \nicefrac{1}{n}
\end{align*}
\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{zipf.png}
\end{figure}

\paragraph{\ild{Elements of language}} We can have different points of view on
language:
\begin{itemize}
\item Phonetics (sound)
\item Grammar
\begin{itemize}
\item Phonology
\item Morphology
\item Syntax
\end{itemize}
\item Semantics (meaning)
\end{itemize}

\paragraph{\ild{Morphology}} How words are built up from smaller meaningful
units, for instance \textit{un-lady-like}, \textit{dog-s}
\begin{itemize}
\item \textbf{\ild{Inflection}} -- variation in the form of a word (usually affix)
  that expresses a grammatical contrast
  \begin{itemize}
  \item adds tense, number, person, mood, aspect, etc
  \item e.g. \textit{run}$\rightarrow$ \textit{run} | \textit{running} 
  \item does not change word class
  \end{itemize}
\item \textbf{\ild{Derivation}} -- formation of a new word from another
  \begin{itemize}
  \item e.g. nominalization (\textit{computer}$\rightarrow$\textit{computerization})
  \item e.g. formation of adjectives (\textit{computational}, \textit{clueless})
  \item changes word class
  \end{itemize}
\end{itemize}

\paragraph{\textit{Morphemes}}
\begin{itemize}
\item \textbf{\ild{Root}} -- equivalence class of a word when all affixes (inflectional and derivational) are removed; not further decomposable into meaningful elements. \iln{The root and the suffixes are morphemes.}
\item \textbf{\ild{Stem}} -- part of word that never changes when
  morphologically inflected, \iln{ i.e. without affixes describing tense,
    number, person, ... } --- obtained by removing inflections
\item \textbf{\ild{Lemma}} -- Base form of word
\item From \textit{produced}, lemma is \textit{produce} but stem is \textit{produc}
\end{itemize}


\section{Tokenization}

\begin{itemize}
\item \textbf{\ild{Token}} A useful semantic unit of the input text, usually a
  single occurence of a word.
\item \textbf{\ild{Tokenization}} Split input (usually text) into sequence of
  tokens. Enables token/word-level analysis, first step for many other methods.
\item \textbf{\textit{Challenges}}
  \begin{itemize}
  \item Keep abbreviations, dates, numbers as single tokens
  \item distuingish abbreviations from words
  \item names and phrases (\textit{queen of england}, \textit{TU Wien})
  \item compound words
  \item apostrophes, umlauts, etc and other linguistic characteristics
  \item encoding issues like RTL/LTR
  \item URLs, numbers, ...
  \end{itemize}
\end{itemize}


\paragraph{\ila{Stop word removal}}
\begin{itemize}
\item \ild{Stop words} -- very frequent and semantically unimportant words
\item Can obstruct/hinder analysis by skewing frequency distribution extremely
  (cf. Zipf's Law)
\item Methods:
  \begin{itemize}
  \item Use predefined dictionairy of stop words
  \item Sort terms by their collection frequency, then remove top $k$ terms
    (maybe in combination with dictionairy.)
  \end{itemize}

\end{itemize}


\paragraph{\ild{Data cleaning}} \gray{(in context of text data)} Methods such as
\begin{itemize}
\item \textit{stop word removal} -- remove frequent and semantically unimportant
  words
\item \textit{prune data} -- remove unneeded, unnecessary, invalid data,
  metadata, formatting, capitalization, etc...
\item \textit{clean data} -- fix formatting, encoding, ...
\item \textit{fix data} -- fix missspelling, incorrect formatting 
\end{itemize}
 



\paragraph{\ila{Maximum Matching algorithm}} Use a dictionary of known terms.
Take the longest prefix of the input string that matches a dictionary item. Does
not always make sense (\textit{``Theta bled own there''}).

\section{Stemming}

\paragraph{\textit{Goal}} Map tokens to equivalence classes in order to treat
different words with very similar semantics as equal (e.g. \textit{runs} and
\textit{run}). In Stemming, do this by remove all inflectional affixes (tense,
number, person, ...)

\paragraph{\ila{Porter Stemmer} } Iteratively removes suffixes of words.
Applicability of rules is based on \textit{measure} of a word $w$, which is the
number $m$ s.t. $w=C(VC)\{m\}V$ where $C,V$ are arbitrary sequences of
consontants, vowels, resp. --- Indeed reduces the words to their \textit{stems}.

\paragraph{\ila{WordNet \textsc{Morphy}} } (\textit{Lemmatizer})
\begin{itemize}
\item Has  •  a sophisticated set of rules about inflections  • exception list
\item Checks the result of transformation against an extensive dictionary
\item Dont get stem but lemma! -- Where \textsc{Porter} gives \textit{leav},
  \textsc{Morphy} gives \textsc{leaf}
\end{itemize}

\paragraph{\iln{Note}} \textsc{Morphy} reduces to \textit{lemmas}, while
\textsc{Porter} reduces to \textit{stems}.

\begin{itemize}
\item \textbf{\ild{Over-stemming}} Two words are reduced to the same root when
  they should not be (e.g. \textit{university, universal} are both reduced to \textit{univers})
\item \textbf{\ild{Under-stemming}} Should be reduced to the same root but are not.
\end{itemize}


\section{POS-Tagging}

Given some input text and some tags (usually word types such as \textit{noun},
\textit{verb}, etc.), want to assign tags to tokens. (\textit{Sequence
  classification problem}).

\paragraph{\textit{Use Cases}} 
\begin{itemize}
\item Parsing of grammatical structure, e.g. dependency parsing
\item Spelling correction
\item Sentiment analysis (e.g. \textit{for the greater good} vs \textit{good
    movie and great actors})
\item Named Entitiy Recognition (e.g. dont classify verbs as NEs)
\item Disambiguation of word senses (e.g. \textit{she saw a bear} vs \textit{the
  tree will bear fruit})
\end{itemize}


\paragraph{\ild{Def.}} Can divide words into two different classes
\begin{itemize}
  \item \textbf{\ild{Closed class}} --- can enumerate all members, e.g.
    determiners, pronouns, prepositions, ...
\item \textbf{\ild{Open class}} --- don't know all members, e.g. nouns, verbs,
  adjectives, ...
\end{itemize}

\paragraph{\iln{Note}}
\begin{itemize}
\item A single term (dictionary entry) can have different optimal POS-tags
  depending on its context.
\item Tagging helps to resolve ambiguities that exist on term-level (.e.g
  \textit{leaves} as \textsc{NN} or as \textsc{VB})
\item Tagging removes unnecessary distinctions e.g. all personal pronouns are
  \textsc{PRP}, determiners
\item Naive method (assigning most frequent tag in training data to term)
  already has 90\% accuracy.
\end{itemize}

\paragraph{\ild{Def.}}
\begin{itemize}
\item \textbf{\ild{Informativeness}} --- Assignment of tag adds information,
  reduces ambuigity
\item \textbf{\ild{Specifiability}} --- Ease of mapping a term to a tag
\item \iln{Example}: Collapsing multiple related tags into one decreases
  informativeness, decreases specifiability.
\end{itemize}

\paragraph{\textit{Feature selection}} Can look at word-local features (term,
pre-, suffixes, capitalization); but very often the tag of a word depends on its
context in the sentence.

\paragraph{\textbf{Main techniques}}
\begin{itemize}
\item \ild{Probabilistic tagging} --- Assign tag based on probability
  distributions of tags in manually tagged training data.
  \begin{itemize}
  \item Need sufficient amount of training data -- no meaningful result if no
    example for tag-word combination is in training data.
  \item Tagging results rely on quality of training data. 
  \end{itemize}
\item \ild{Rule-based tagging} --- use rules based on linguistic understanding
  \begin{itemize}
  \item rules need to be created manually, need expert-knowledge and time
  \item need a large number of rules to be effective
  \item good to tailor solution to very specific problems
  \end{itemize}
\end{itemize}

\paragraph{\ila{Backoff Tagger} } Combine different taggers (e.g. higher-order
first, then lower)

\subsection{Probabilistic tagging}

\paragraph{\ild{$n$-gram language models}} Consider the definition of {
  conditional probability }
\begin{align*}
  P(A|B) = \frac{P(A,B)}{P(B)} 
\end{align*}
This gives rise to the \ild{chain rule}
\begin{align*}
  \Rightarrow P(A,B) = P(A|B) \cdot P(B)
\end{align*}
or, more generally
\begin{align*}
  P(w_1, ..., w_n) = \prod P(w_i~|~w_1, ..., w_{i-1})
\end{align*}
The problem here is that we cannot realistically obtain all the components of
the product because there are way too many possible sequences of words. Instead,
we employ the \ild{Markov assumption} that says that we can estimate the
probabilities by only consiering only the $k$ preceding terms
\begin{align*}
  P(w_i ~|~ w_1 ..., w_{i-1}) \approx P(w_i ~|~ w_{i-k}, ..., w_{i-1})
\end{align*}

For $k=1$, this yields the \ild{unigram model}, for $k=2$ the \ild{bigram model}
(i.e. $P(w_i | w_{i-1})$).

$n$-gram modelling is insufficient because language has \textit{long-distance depenencies}.

\paragraph{\ila{Unigram Tagger} } \gray{ (How probable is each tag for a given word?) }
Assume that a unigram model generates the current tagging.

Assign a token $w$ its most frequent tag, i.e.
\begin{align*}
  t(w) := \argmax_t P(t~|~w)
\end{align*}
Use Bayes' formula to calculate probabilities, i.e. $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$,
omitting the quotient:
\begin{align*}
  t(w) := P(t~|~w) = \argmax_t P(t) \cdot P(w~|~t)
\end{align*}
Considering the estimate the probabilities by their frequency in the training
set, this can be simplified even further:
\begin{align*}
  P(w|t) = \frac{ \mathit{count}(w,t) }{\mathit{count}(t)} \\
  P(t) = \frac{  \mathit{count}(t)}{n}
\end{align*}
where $n$ is the number of words, equal to the number of assigned tags; when
trying to find the argmax $t$, we dont even have to consider the denominator:
\begin{align*}
  P(t|w) \approx P(w|t) \cdot P(t) \\
  = \frac{\mathit{count(w,t)}}{n} \approx \mathit{count}(w,t)
\end{align*}


\paragraph{\ila{$n$-gram tagger} } Use information about the previous $n$
tokens in addition to information about current token. Can have \ild{word-based}
and \ild{tag-based} (tags are more common, training data covers more ground).

Assume a bigram language model (generating the sequence of POS-tags).

Pick the tag $t_i$ for word $w$ that maximises
\begin{align*}
  P(t_i ~|~ t_{i-1}) \cdot P(w~|~t_{i})
\end{align*}

For finding $P(t_i~|~t_{i-1})$, use the \ild{Maximum Likelihood Estimate} (where
$c$ is the count of observations)
\begin{align*}
  P(t_i | t_{i-1}) \approx \frac{\mathit{count}(t_{i-1} \text{ then } t_i)}{\mathit{count}(t_{i-1})} \\
  P(w | t_i) ~~ \text{same as in unigram model} 
\end{align*}
\begin{itemize}
\item {(Use start and end symbols to be able to calculate probs for item first
    and last words)}
\item Problem: Large memory requirements to store $n$-grams (very sparse data structures)
\end{itemize}

% For $n=2$, can imagine a twodimensional lookup table:
% \begin{align*}
%   \text{previously seen POS-tag} \times \text{current word} \rightarrow 
% \end{align*}
% bigram table gives most frequent tag (based on training set)

\paragraph{\iln{Note}} These sequence-based taggers/models have the problem that if the
tag for the preceding word cannot be determined, this influences the performance
for the current word.


\subsection{Rule-based tagging}

Try to incorporate linguistic insight.

\paragraph{\ila{Feature-based tagger} } Only consider features of single word to
be lagged. Look at word, prefixes, suffixes, capitalisation.

\paragraph{\ila{Brill tagger} }
\begin{enumerate}
\item Tag each word using a \ild{ baseline tagger } (e.g. unigram tagger, i.e.
  most common tag)
\item Apply patches that improve the result
  \begin{itemize}
  \item e.g. \textit{if one of the two preceding words is a determiner, change
      the tag from verb to noun}
  \item Based on training data, compute the error between any two
    should-be/is-assigned: $(t_a, t_b, \mathit{freq})$.
  \item For each error triple, apply the patch that results in the greatest
    improvement, apply it.
  \item Repeat until no further improvement is possible.
  \end{itemize}
  The patches are learned from a training corpus.
\end{enumerate}



\section{Parsing}

We try to determine the grammatical structure of a sentence.
\begin{itemize}
\item \textit{Constituency parsing} yields parse tree according to a given
  formal grammar -- useful when subtrees are interesting (e.g. spelling correction)
\item \textit{Dependency parsing} only yields pairwise relationships between
  words -- useful when these are interesting, e.g. we want to identify modifier
  words (sentiment analysis, question answering)
\end{itemize}

\begin{itemize}
\item Top-down approach -- Try to generate sentence
\item Bottom-up approach -- Try to contract tokens into nonterminals
\end{itemize}



\paragraph{\ild{Parsing problem}} Identify the parse tree (w.r.t some grammar)
for a given sentence. \iln{(There may be multiple parse trees, or none.)}

\paragraph{\ild{Parse tree}} depicts derivation of sentence beginning from start
symbol. Obviously requires context-free grammar \iln{(each set of children has
  exactly one parent)}

\paragraph{\textit{Motivation}} Grammar checking, Question answering, Machine
translation, ...

\paragraph{\ild{Ambiguous grammar}} has more than one \textit{leftmost}
derivation parse tree.

\subsection{Constituency parsing}

\paragraph{\textit{Basic assumption}} Language is made up of \ild{constituents},
i.e. basic, nested building blocks \iln{(terminals and nonterminals of a formal grammar.)}

\paragraph{\ila{Leftmost derivation}} \gray{(top-down)} Always expand the leftmost expandable
nonterminal. Potentially lots of backtracking applied. Always yields unique
parse tree (if grammar is unambiguous).

\paragraph{\ila{Shift-reduce parser}} \textit{(bottom-up approach)} Push tokens
onto stack until top of stack matches the \textit{rhs} of a rule, then reduce
(on stack). Accepts the word if start symbol alone is left on the stack.

\paragraph{\ild{Chomsky normal form}} The \textit{rhs} of a rule is of one of
these shapes
\begin{itemize}
\item $A \rightarrow BC$
\item $A \rightarrow a$
\item $S \rightarrow \varepsilon$
\end{itemize}

\todo transform grammar into CNF


\paragraph{\ila{CYK Parser} } \textit{(dynamic programming, bottom-up)} Assumes
a grammar in CNF, i.e. \textit{rhs} of rules is two nonterminals or a terminal.
From the bottom up, in a dynamic programming fashion, remember if two subcells
are the \textit{rhs} of a production rule. The topmost cell produces the entire sentence.
\begin{figure}
  \centering
  \includegraphics[width=0.4\linewidth]{cyk-parsing}
\end{figure}
Note that this finds all possible parse trees.
\iln{($\rightarrow$ Assignment 5)}
\begin{itemize}
\item Potentially quadratic space complexity
\item Finds all possible parse trees (and all possible parses for substructures)
  which might not always be needed.
\end{itemize}


\subsection{Statistical Parsing}

\textit{Motivation: Sentences can have many different parse trees but some are
  clearly more likely than others.}

\paragraph{\textit{Basic idea}}
\begin{itemize}
\item Associate rules of a grammar with probabilities
\item The probability of a parse tree is the product of all used
  productions/derivations.
\item Probability of a sentence is the sum of all possible parse tree
  probablities.
\item Probabilities can be learned from a training corpus.
\item Can use probabilites in parsing algorithms such as the CYK parser to find
  the most likely parse tree.
\end{itemize}

\paragraph{\ila{Lexicalised Parsing}} Extend production rules to be specific to
head word of sentenc (e.g. \textit{VP(ate)  $\rightarrow$ VP(ate) PP(with)}). Then, again,
assign/learn probabilities. \todo Head of sentence

\paragraph{\ila{Shallow Parsing} } Simply break up text into (nested) chunks.

\subsection{Dependency Parsing}

\paragraph{\textit{Basic concept}} Dependencies (in the linguistic sense)
between tokens (such as \textit{determiner, subject, ...}), these form a tree.
Again, want to find the most likely parse/dependency structure

\paragraph{\ila{Constraint-based Dependency Parsing} } Come up with rules describing what a
dependency can possibly look like. Begin with a complete graph of pairwise
dependencies, then iteratively eliminate dependencies according to rules.

\paragraph{\ila{Global linear models} } Assign a \ild{local feature vector}
$g(x,h,m)$ to a specific dependency $(h,m)$ in a specific sentence $x$. Try to
maximise the (weighted) sum of all depenencies in the sentence, i.e. the best
parse $y = F(x)$ is given by
\begin{align*}
  F_y(x) = \argmax_y \vec w \cdot \sum_{(h,m)\in y} g(x,h,m)
\end{align*}
where $\vec w$ is a weight vector for components of $g$; for binary features it
can be derived from a given set of training parses by $\frac{\text{\# matching
    arcs for $i$} }{\text{\# total arcs} }$. $g_i$ are features/scorings for a
given input/test sequence.
\begin{itemize}
\item Advantage: nicely generalisable, can e.g. incorporate semantics of word
  into feature vector.
\item Possible features/scorings:
  \begin{itemize}
  \item Dependency is between POS-tags that are likely related, e.g. noun
    and verb
  \item Dependency is of short range
  \end{itemize}
\end{itemize}


\paragraph{\ila{Perceptron algorithm} } As per Assignment 6, this is simply
accepting the dependency structure with the higher score $F_y(x)$.


\pagebreak
\part{Similarity Measures}

\textit{As there are many different expressions for the same semantic concept,
  we want to derive concepts of similarity between words. Further, this is
  useful in spelling correction. Focus on phonological and spelling similarity.}

\section{Word similarity}

\paragraph{\ild{Phonological similarity}} How much do two words sound alike?

\paragraph{\ila{Soundex} } Reduce a given term/word into a phonetic
representation based on rules.

\paragraph{\ild{Morphological similarity}} basically comes down to stemming.

\paragraph{\ild{Spelling similarity}} Mainly want to identify spelling errors

\paragraph{\ila{Levenshtein distance / edit distance}} (\textit{dynamic
  programming}) We already know this.
\begin{itemize}
\item Advantages: simple, easy to implement, finds all optimal alignments,
  flexible by employing different selection mechanism in dynamic programming.
\item Quadratic time and space complexity in its simple implementation
\item Character-based, does not consider local or sentence-scope context.
\item Extensions: Cost matrices, based for example on distance of letters on
  keyboard.
\end{itemize}
\paragraph{\ild{Semantic similarity}} 
\begin{itemize}
\item \ild{Synonym} -- same meaning. A \ild{synset} is the equivalence class of synonymity.
\item \ild{Antonym} -- opposite meaning
\item \ild{hypernym} -- more general meaning
\item \ild{hyponym} -- more specific
\item \ild{meronym} -- part of collective
\end{itemize}

\paragraph{\ila{WordNet}} provides a tree (forest) of semantic relationships
between words. Similarity between $v,w$ can then be defined e.g. \todo


\section{Document similarity}

\paragraph{\ild{Vector space model}} See IR lecture notes.

\paragraph{\ild{tf-idf weight}} \begin{itemize}
\item \ild{term frequency $\mathit{tf}(t,d)$} --- How often does term $t$ appear in
  document $d$? Commonly dampened by logarithm.
\item \ild{document frequency $\mathit{df}(t)$} --- In how many documents does $t$ appear?
\item \ild{inverse document frequency $\mathit{idf}(t)$}  ---
  The basic idea: Rare terms are more informative
  than frequent terms and should thus receive a high score.  Thus we do the
  inverse fraction and log-dampening:
  \begin{align*}
    \mathit{idf}(t) := \log \frac{n}{\mathit{df}(t)}
  \end{align*}
\end{itemize}
The tf-idf weight is given by the product of term frequency and inverse document
frequency:
\begin{align*}
  \mathit{tf.idf}(t,d) = \log(1 + \mathit{tf}(td)) \cdot \log \left( \frac{n}{\mathit{df}(t)} \right)
\end{align*}
\begin{itemize}
\item term frequency increases with the number of occurences in the document
\item inverse document frequency increases with the rarity of the term in the collection
\end{itemize}

The score for a multi-term query and a document is
\begin{align*}
  \mathit{score}(q,d) := \sum_{t \in q \cap d} \mathit{tf.idf}(t,d)
\end{align*}
Problems:
\begin{itemize}
\item Bag-of-words model, does not consider order
\item Score is a sum, longer documents receive higher score
\end{itemize}

\paragraph{\ild{Vector space model}} Each document is a vector consisting of
tf-idf weights of terms. \ild{Distance measure}:
\begin{itemize}
\item euclidean distance is not suited because it considers the lengths (norms)
  of vectors, but we only really care about the distribution of terms
\item hence use \ild{cosine similarity}, which measures the angle between
  vectors. \todo calculation
\end{itemize}

\paragraph{\ild{Cosine similarity}} Consider the formula for the euclidean dot
product:
\begin{align*}
  a \cdot b = ||a|| ||b|| \cos \theta,
\end{align*} this yields for two vectors $a$ and $b$
\begin{align*}
  \text{sim(a,b)} = \cos \theta := \frac{a \cdot b}{||a|| ||b||} 
\end{align*}
where $a \cdot b = a^Tb$ is the dot product, $||a|| = a \cdot a = a^Ta$



\pagebreak
\part{Language Modelling}

\paragraph{\ild{Basic pipeline}}
\begin{enumerate}
\item Corpus
\item high-dimensional vector space
\item lower-dimensional latent space (word embeddings)
\item word relationships
\end{enumerate}


\paragraph{\ild{Word embedding}} ``Embed'' words in a vector space $V$ s.t. similar
word-vectors are similar w.r.t some measure in $V$. Motivation: Approaches like WordNet are static and thus
insufficient. Possible information
\begin{itemize}
\item syntactic similarity (\textit{run, ran})
\item semantic similarity (\textit{large, big})
\item relatedness (\textit{coffee, cup})
\end{itemize}

\paragraph{\ild{Word context matrix}} (\textit{co-occurence matrix})
\begin{itemize}
\item Each row represents a word
\item Each column represents some ``context'' (specific entity or other word)
\item cell represents the strengh of association, for example \textit{pointwise
    mutual information}
\end{itemize}
Basically the word-vectors put next to each other. This matrix is very sparse.
We'd like to find a low-dimensional representation of our word vector (word
embedding). We project into a lower-dimensional space, the so-called \ild{latent
  space}.

\paragraph{\ila{Word2Vec}} \gray{(neural network for learning word vectors,
  context-independent model)}
Given a training corpus, every word in a fixed vocabulary is represented by a vector.
\begin{enumerate}
\item For each center word $c$ and a context window of fixed size $o$...
\item use the word vector similarity of $c$ and $o$ to calculate $P(o|c)$ (or
  $P(c|o)$, see below)
\item Adjust word vector to maximise predictive accuracy
\end{enumerate}
\begin{itemize}
\item \ild{Continous Bag-of-Words} (CBOW) learns $P(c|o)$, i.e. focus word given
  the context
\item \ild{Skip-gram} learns $P(o|c)$, i.e. context given some focus word.
\end{itemize}
Limitations:
\begin{itemize}
\item Any given word is represented by a vector which has to be stored
\item Context-independent, outputs always only one vector for a word: Polysemy
  (different meanings of same word) is not adressed at all
\item Dependence of meaning on context is not considered (gives rise to
  \ild{context-dependent models})
\end{itemize}

\paragraph{\ila{\textsc{Bert}} } example for a context-dependent model, based on
a neural network.

Given some word embeddings, how do we determine their similarity?
\begin{itemize}
\item similarity measures like cosine similarity
\item visualise by dimension-reduction techniques like t-SNE, PCA, MDS, UMAP,
  ...
\end{itemize}

\begin{itemize}
\item \ild{context-independent models} output a single word vector for a word
\item \ild{context-dependent models} generate multiple word embeddings for a
  word that capture different contexts.
\end{itemize}

\paragraph{\ild{Sentence embedding}} Basic approach with Bag-of-words
assumption, sentence is represented by vector of tf-idf weights
\begin{itemize}
\item Motivation: Meaning of words is often defined by the entire sentence, this
  takes these larger structures into account
\item  Problems:
  \begin{itemize}
  \item vectors are large and very sparse
  \item cosine similiarity depends only on components, i.e. single words.
    Sentences might have a $\cos \theta$ of $0$ but still be semantically similar.
  \end{itemize}
\item Alternatively, to represent a sentence, use the average of all its word
  embeddings. Disadvantage: Long sentences have less meaningful average.
\item Deep learning approaches
\end{itemize}

\paragraph{\textit{Applications}}
\begin{itemize}
\item Assistants like Google, Siri, Alexa, ... for understanding an generating
  language, answering questions, ...
\item Opinion mining
\item Sentiment analysis
\item Named Entity Recognition
\end{itemize}

\paragraph{\textit{Limitations}} Language models potentially contain biases
(ethnic, gender) induced by the training data.


\pagebreak
\part{Text Data Mining}

\section{Text classification}
\paragraph{\textit{Applications}}
\begin{itemize}
\item categorise web pages
\item spam vs. non-spam
\item categorisation of correspondence
\item categorisation of news articles
\item classify to author
\item classify reviews
\item easy/hard to read, suitability for age groups
\item writing style
\end{itemize}


Choice of classification algorithm is dependent in characteristics and amount of
training data.

\paragraph{\textit{Feature selection}}
\begin{itemize}
\item tf-idf weights of terms
\item phrases, characters, $n$-grams
\item POS-tags
\item metadata about the document
\item non-text parts of document
\end{itemize}


As always, have a rule-based and a machine learning approach

\paragraph{\ila{Naive Bayes} } \gray{ (for text classification) } Assumes
bag-of-words (independence between words). For a class $c$ and words $w_i$, find
the predicted class $y$ by
\begin{align*}
  y = \argmax_c P(c) \prod_i P(w_i~|~c)
\end{align*}
For individual probabilities use maximum likelihood estimate, i.e.
\begin{itemize}
\item $P(c) = \frac{\text{\# of training docs with class $c$}}{\text{ \# all docs }}$
\item $P(w_i| c) = \frac{\mathit{count}(w_i, c_j)}{\sum_w \mathit{count}(w, c_j)}$
\end{itemize}
\textbf{Important:} Use laplace smoothing here, i.e. with $|V|$ being vocabulary size:
\begin{align*}
  P(w_i| c) = \frac{\mathit{count}(w_i, c_j) + 1}{\sum_w \mathit{count}(w, c_j) + |V|}
\end{align*}


\section{Text clustering}

\textit{Want to put documents into groups that are similar within and dissimilar
  across groups. Usually unsupervised, i.e. no labels given in advance.}

\paragraph{\textit{Feature selection}} Similar to classification.


\section{Topic modelling}

Possible improvements/preprocessing:
\begin{itemize}
\item Only keep nouns (per POS tag)
\end{itemize}

\paragraph{\ila{Latent Dirichlet Allocation (LDA)} } Try to find distributions that
characterise topics (distributions over words) that, in some mixture, best
generate/explain a given document. In other words, assume the document to come
from a distribution that is a mixture of topic distributions. 
\begin{itemize}
\item Need to give a fixed number of topics as parameter
\item A single keyword can be part of multiple topics (with a specific
  probability each) because a topic is a distribution over words.
\item Compared to basic text clustering, topic modelling yields probabilities
  for mapping document $\mapsto$ topic, clustering (most of the time) yields
  binary assignment. Clustering via tf-idf vectors only considers word
  frequencies whereas LDA considers correlations \todo
\item Disadvantage: Does not consider word order
\item Should do stop-world removal, lemmatization because these skew the distributions
\end{itemize}


\pagebreak
\part{Opinion Mining \& Information Extraction}

\section{Information Extraction}

To extract and structure relevant information from unstructured (text) data. In
other words: the curation of structured representations that are easily
accessible while maintaining semantic information of original text.

Here, we focus only on factual information (\textit{What year was Francis Bacon
  born in?}) and not opinions.

\subsection{Named Entity Extraction}

Used for  • summarizing text  • answering questions  • integrating into knowlege
bases  • associating information (e.g. opinions) to sentiments (e.g. of parts
of printer in question)

Possible types of entities  • location  • time  • person  • organisation etc.
which can stand in various relations to each other. 

\paragraph{\textit{Supervised learning models}} Based on labelled training
sequences (of tokens), train a classifier to predict labels (\textit{Sequence
  Labelling Problem})
\begin{itemize}
\item new data must fit training data
\item time-consuming
\end{itemize}

To make this easier, use features that go beyond single tokens, e.g. context
window of $k$ words.

\paragraph{\textit{Sequence Labelling}}  • reminiscent to POS-tagging  • assuming
that label is dependent on context. Typical models are  • Markov models  •
Conditional Random Fields  • Bidirectional LSTMs

Once we have identified the entities, we'd like to find relationships between
them (e.g. triples of operators \textit{is-a}, \textit{daughter-of}, ...)
\iln{(Can save these triples in a knowledge base e.g. for question-answering; cf
  \textsc{Rdf}-triples)}.

\subsection{Entity Relation Extraction}

Try to find type of relationship between two entities (e.g. \textit{X}
is-daugher-of \textit{Y}). Possibilities:
\begin{itemize}
\item Extract \textsc{Rdf} triples from large corpora like Wikipedia
\item Use (specialised) ontologies / knowledge bases (for ex. medical applications)
\end{itemize}
Methods to extract information:
\begin{itemize}
\item handwritten rules -- e.g. \textit{``Y such as X'', ``X, especially Y'',
    ``X, including Y'' all express an \textit{is-a}-relationship}. -- there can
  be more specific relations that only make sense between certain types of
  entities (e.g. \textit{cures(drug, disease)}) -- pros:  • precise  • can be
  tailored to specific domains -- cons:  • low recall  • high effort
\item supervised --- define features and ground-truth tags/labels, then apply any
  classifier -- pros: high accuracy if enough training data -- cons: does not
  generalise well
\item unsupervised machine learning.
\end{itemize}



\section{Opinion Mining / Sentiment Analysis}

\paragraph{\textit{Applications}}
\begin{itemize}
\item Classifying reviews (obtain user review information about a product) --
  gain information on product, or fix contradicting reviews
\item In recommender systems (recommend items with similar sentiment)
\item Identify subjectivity (useful in information extraction)
\item Actually return opinions when requested (politics, sociology, ...)
\item Identify opinion holders
\end{itemize}

\paragraph{\ild{Terminology}}
\begin{itemize}
\item \ild{Subjectivity Analysis} -- determine whether statement is subjective
  or not (done at sentence level)
\item \ild{Sentiment Analysis} -- classify documents according to their polarity
  (sentence or document level)
\item \ild{Opinion Mining} -- extract attributes and opinions
\end{itemize}

Useful preprocessing steps: Named entity recognition and dependency pasing to
identify which sentiment modifies relate to which entities in the text. 

\subsection{Opinion Mining at Document level}



\paragraph{\ile{Example: Sentiment classification}} Consider only presence (not
counts) of adjectives in text. Tag adjectives if negated. Disregard nouns,
proper nouns because they should be irrelevant for sentiment polarity.

For instance, we can use our well-known naive bayes for classification

\begin{align*}
  y = \argmax_c P(c) \cdot \Prod P(w_i|c)
\end{align*}


\paragraph{Basic assumption} Each document focusses on a single object and
contains opinions from a single opinion holder.

\paragraph{\textit{Problems}} with this approach
\begin{itemize}
\item Dont extract \textit{what} the sentiments are about
\item Might express different sentiments about different aspects of the object.
\end{itemize}

\subsection{Attribute-based Opinion Analysis}

\begin{enumerate}
\item POS-tagging
\item Frequent feature generation -- try to identify features (attributes) in
  the text (e.g. \textit{zoom})
\item Prune features
\item Identify infrequent features by considering the head of opinion phrases to
  be features
\item Identify semantic orientation -- using WordNet for each opinion word, then
  average to obtain orientation for entire sentence
\end{enumerate}
Output: Sentences and their Polarity by Feature.

Challenges
\begin{itemize}
\item Subtle language
\item Context-dependent attributes (i.e. \textit{the image is very clear})
\item Strength of opinion
\item Figures of speech, thwarted expectations narrative (\textit{it should have
  been good but it wasn't})
\end{itemize}


\subsection{Lexicon construction}

\paragraph{\ila{Dictionairy-based techniques} }
\begin{enumerate}
\item Start from a seed-set of positive (\textit{good}, \textit{nice}, ...) and
  negative (\textit{bad}, \textit{terrible}, ...) opinion words.
\item Lookup synsets (synonyms) and antonyms in WordNet.
\end{enumerate}
Disadvantage: Does not find context-dependent opinion words (\textit{sharp,
  small, long, ...})

\paragraph{\ila{Corpus-based techniques} }
\begin{enumerate}
\item Start from seed set (like above)
\item In training corpus, try to find linked adjectives -- linked by e.g.
  \textit{and}, \textit{which is...}, \textit{not only X but also Y}
\end{enumerate}


\section{Beyond Sentiment}

Identify...
\begin{itemize}
\item Emotion
\item Mood
\item Interpersonal stances
\item Personality traits
\end{itemize}


\pagebreak
\part{Question Answering \& Text Summarization}

\section{Question Answering}

\subsection{Information-retrieval-based factoid question answering}

\subsection{Knowledge-based question answering}

\section{Text Summarization for Question Answering}

\subsection{Single-document}

\subsection{Multi-document}




\end{document}